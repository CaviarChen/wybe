%  Emacs    : -*- latex -*-
%  File     : design.tex
%  RCS      : $Id: design.tex,v 1.11 2007/11/20 12:13:19 schachte Exp $
%  Author   : Peter Schachte
%  Origin   : Fri Mar 28 19:36:48 2003
%  Purpose  : Discussion document on design of new language
%  Copyright: © 2003 Peter Schachte.  All rights reserved.
%

\documentclass{article}
\usepackage{xspace}
\usepackage{ensuremath}

\newcommand{\frege}{\textsc{Frege}\xspace}
\newcommand{\Frege}{\textsc{Frege}\xspace}

\newcommand{\ie}{\emph{i.e.}\xspace}
\newcommand{\eg}{\emph{e.g.}\xspace}
\newcommand{\Ie}{\emph{I.e.}\xspace}
\newcommand{\Eg}{\emph{E.g.}\xspace}
\newcommand{\etc}{\emph{etc}}

\newcommand{\tuple}[1]{\ensuremath{\langle#1\rangle}}

\title{The \Frege Programming Language}
\author{Peter Schachte}

\begin{document}
\maketitle

\section{Aims}

The aim of this project is to produce a pure, clean declarative
programming language with object oriented and imperative features that is
simple and intuitive enough to be suited to 
teaching as a first or second language, yet powerful enough for
serious software engineering.
To fit the range of intended audiences, the language must
restrictable:  it must be possible to use subsets of its features without the
interference of other features.
Likewise, the language should minimize the number of decisions the user is
required to make, but maximize the number of decisions the user is permitted
to make.

Also in the interests of less experienced users, the language makes heavy use
of polymorphism.  This means there is less for the user to remember:  if a
particular operation works on one datatype, the same operation should work
similarly on all similar types.  It also benefits the experienced programmer,
since a single implementation will work for many types.  There is, however,
an efficiency concern, since different types have different performance
characteristics, suggesting different implementations.  Again, the language
should be prepared to apply a single algorithm to different types, but should
permit the programmer to specify a different algorithm if desired.

From the standpoint of software engineering, the language must support
separation of concerns:  it must be possible to define each aspect of a
system in a separate software component.  Furthermore, each aspect should be
separately maintainable and, where appropriate, reusable.

In an egregious instance of acronym reverse engineering,
the current working name for the language is \frege, for
\textbf{F}unctions, \textbf{RE}lations, and pervasive \textbf{GE}nerics.


\section{Types / Classes / Modules}

The language should be strongly typed, with parametric polymorphism
and type inference.

All data types should be abstract.  This would be achieved by treating
constructors (and deconstructors) as ordinary functions.  Thus an
existing type with defined constructors and deconstructors could be
redefined with new functions implementing the previous constructors
and deconstructors.

A type can then be viewed as just the set of functions and predicates that
operate on it.  That is, a type is just a kind of module.  One difficulty
with this view is that many operations involve multiple types, so each
operation may belong in multiple modules, while naturally each operation must
belong only to a single module .  This, presumably, is why object oriented
languages single out one argument to ``receive'' a message, with any other
arguments just coming along for the ride.  This asymmetry is rather
unpleasant, doesn't fit well with higher-order programming, and should be
avoided if possible.

\Frege supports two kinds of modules:  \emph{classes} and \emph{modules}.
A \texttt{class} defines a single public type, though it may also define
private types.  A \texttt{module} defines no types; it only bundles a
collection of operations.

It should be
possible to define nested modules in a layered kind of way: the
innermost layer would be just the constructors or most primitive
operations.  Only layer 1 would be able to use layer 0 operations, and
only Layer 2 would use layer 1, \etc.  This would allow maintenance on
one layer at a time, with changes to any layer not affecting the lower
and higher levels, as long as the same interfaces can be maintained.

\Frege supports three approaches to module composition:  embedding, extension,
and delegation.  Embedding one module within another hides it from any other
module.  When the interface of an embedded module is changed, only the
embedding module needs to be updated.  The price of this protection is that
no other module may directly use the inner module.

Extending a module defines a new module that provides the full interface of
the extended module, possibly plus some extra features.  This is akin to
inheritance in object oriented languages, and indeed extension allows the
extension module to override features of the extended module by defining its
own implementations.

Delegation permits a module to conveniently specify that features of one
class should also be considered features of a new class.  This can be done by
specifying a coercion function to map an instance of the new class to the
existing class.  Note, however, that for delegated features to be reversable,
the coercion function must be reversible.  

Pushing these ideas a little further, it is also possible to treat any
module as an interface that any other module could be specified to
implement --- \ie, interface inheritance, or type classes.  This should
make it possible to define a new list module that implements a list as a
packed array, and have all predicates and functions that operate on lists
also work on that type.  It should even
be possible to define input and output streams to be compatible with
lists, so list operations should work for them too.  Read from or
write to a stream by just appending to or from it.

Thus a module may declare that it \texttt{extends} or \texttt{implements}
another module.  Also, a unary function may declare that it
\texttt{delegates} a number of features.  These features must be members of
the range of that function, and are thereby also made features of the current
module.  Finally, a module may be declared inside another module, in which
case the features of the inner module cannot be used outside of the outer
one.  However, any features defined in the inner module, or defined through
delegation, may be made public by the (outer) module.

Understanding determinism is important for this to work, though.
Therefore, there would have to be a mechanism for declaring mutual
exclusion and exhaustiveness of functions (and predicates).

At the lowest level, it would be necessary to be able to access the
raw data of the type.  Most types would be defined as algebraic types,
but sometimes something more low-level would be best.
In fact, the algebraic type facility should be defined in a
library; users could define their own type facilities.  \emph{E.g},
there could be a C-compatible type system in a library, which would
lay out types compatibly with the way C does, allowing a C type to be
treated as a native \frege type.

Probably the
most flexible primitive building block would be the \texttt{bits}($n$)
type, specifying a packed array of $n$ bits.
This would allow a Boolean or character type to be 1 or 8 or 16
bits.  There would also need to be an \texttt{address} type, at least
large enough to
hold the address of an address, and a addressable
array of bits, which would be aligned as necessary to make it
efficiently addressable.  This should allow lists of characters to be
represented as packed arrays of byes.

There should be a sizeof function returning the size of any type.  In
particular, this can be used to find the size of an address.  
This needs to be
generalised to allow different approaches for addressing characters
and integers/addresses.

Garbage collection would also need to be considered in the design.
The garbage collection interface for the low level facility should be
kept as flexible as possible to allow users to do various clever
things with foreign data, and still allow different implementations to
use different garbage collection strategies.

Each type should be able to specify how instances should be treated
when used in a context where a function or predicate is expected.
This would allow Boolean-valued functions to be used as predicates,
since \texttt{true} used as predicate could be defined to succeed and \texttt{false}
to fail.  It also gives a nice approach to arrays, as an array type
could be defined to apply indexing when used as a function.  That
doesn't handle array element assignment, though.  How can this idea be
generalised?  Perhaps as a coercion system where users can define how
to coerce their type to any other type, including function and
predicate types?  Maybe they want to define how to coerce any other
type to theirs?  But coercion seems directional, which could be a
problem in a logic language.

The language itself should be simple.  There should be very little built in;
most of it should be in libraries.  Ideally, all datatypes that would
normally be considered primitive would be provided by libraries,
which would implement them through the foreign interface.
Probably function and predicate
(higher order) types would have to be built in.  This raises
interesting issues for manifest constants.  If integers aren't built
in, how can the compiler handle constants like 42?  You can't define
the integers as an enumeration!  This could be handled by allowing each type
to define a reader method to convert from a token to a member of the type.

One solution would be to define the language lexically to consist of a
sequence of tokens, where a token is a sequence of characters.  Each token
would be one of the following:
\begin{itemize}
\item one or more alphanumeric characters (including underscore);
\item one open or close parenthesis, bracket, or brace character;
\item a single or double quote or backquote, followed by any number of
  characters up to the matching quote; or
\item a \emph{here} string, described below; or
\item one or more punctuation characters (anything but whitespace not covered
  above).
\end{itemize}
Within a quoted token, a backslash character is treated much as in C:  if the
following character is a letter, it is a character escape; otherwise it is
treated literally; in particular, backslash may be used to escape a quote
character.  Also, a backslash as the last character on a line indicates that
the end-of-line and all following whitespace characters should be ignored.

A \emph{here} string begins with a \emph{here quote} and ends with an
identical here quote, with all intervening text completely uninterpreted.  A
\emph{here quote} is any sequence of non-whitespace characters enclosed
between backslash characters.

This approach would create problems for type inference:  manifest
constants would no longer indicate their types.

\section{Modes}

One major principle here is that modes should be simple for users to
understand.  Generally, the mode system should only be responsible for
correctness, not performance.

In the face of CLP, the meaning of groundness and determinism become
unclear.  What does remain clear and consistent, however, is the concept of 
\emph{unique determination} of a value.  Thus a predicate that backtracks
over the values of 0 and 1 for \texttt{X} is treated identically to one that
constrains \texttt{X} to be either 0 or 1.  This permits the compiler to take
either approach.

Thus \frege's mode system is based on dependencies among unique determination
of values.  \Eg,
\texttt{append}/3 would specify that if any two arguments are uniquely
determined, the so is the third.

Modes may also be responsible for determining the required initial
instantiation state for a predicate.  For example, the initial
instantiation for the \texttt{plus}/3 predicate (and the \texttt{+}/2 function) might
specify that two of the three arguments must be ground on call.

It may make sense to follow Mercury in requiring the modes of exported
predicates/functions to be declared.  Others should be inferable.

It should be possible to write different code for different modes of a
predicate.

Perhaps it should be possible to specify what Java would call static
data members for types, \ie, data lexically scoped to the type but
having the lifetime of the program.  For example, to implement a CLP
real variable
type, one needs a constraint store.  To do this properly, this
constraint store needs to be threaded throughout the code.  This leads
to the wrong interface for the primitive operations:  \texttt{+} would
relate three real variables and two constraint stores.  From the
standpoint of modularity, the constraint store should be considered
local to the implementation of the real type.  This gives the
primitive operations the desired interface.  However, we need some
assurances that allowing this will not affect the desired properties
of conjunction; \eg, \texttt{D = A + B, E = D + C} should behave the
same as \texttt{E = D + C, D = A + B}.

An extension of this idea would be to permit conjunction to lose the
desired properties, but then require the use of a special syntax when
using those predicates (perhaps calls to such predicates would have to
be enclosed in braces).  This would give a form of hidden state
threading.  It would be nice to use this for grammars \emph{\`a la}
DCGs, but there would need to be a way to define types that hide state
and make different predicates use different ones of these types.  \Eg,
some clauses would be written as normal 2-extra-argument DCGs, but
others may want to be written as 4-extra-argument DCGs producing an
output text and allowing substitutions.  Perhaps type inheritance
could do this:  define the grammar inside a type that ``extends'' the
DCG type or the substitution grammar type.


\section{Determinism}

Determinism can be at least partially handled by a functional
dependency analysis.  This could be handled by having an instantiation
\texttt{definite}, meaning ground and lacking any alternatives, which is more
specific than \texttt{ground}.  A predicate's determinism could then be
specified as part of its mode.  The determinism of \texttt{append(A,B,C)}, \eg,
could be specified as ``any one is definite if the other two are.''
This, however, doesn't handle possible failure.  For nonfailure, we'd
want to say that \texttt{append(A,B,C)} may fail if both \texttt{B} and \texttt{C} are bound, or
if both \texttt{A} and \texttt{C} are bound.

Note also that this doesn't really tell us everything we want to know
about choicepoints.  For example, this doesn't tell us that
\texttt{append(A,B,C)} will never leave a choicepoint if the spine of \texttt{A} is
fully bound.  Whether \texttt{B} is bound or not is immaterial to choicepoints.

However, it is more powerful than the Mercury system.  It allows us to
consider some arguments of a predicate to be deterministic.  For
example, the first argument of

\begin{verbatim}
	p(a, b).
	p(a, c).
\end{verbatim}

is definite, while the second is not.  We do not need to store the
first argument in a choicepoint or rebind it when we backtrack into
\texttt{p}/2.  This approach also is not attached to modes; a predicate would
have a single formula describing its determinism.  Furthermore,
definiteness of answers is probably a more natural concept to users
than lack of choicepoints.


\section{Syntax}

Since one of the aims of this language is to reconcile object oriented with
functional and logic programming, \frege syntax treats \texttt{x.f(a,b)} as
exactly the same as \texttt{f(x,a,b)}.  One consequence of that is that infix
\texttt{.} is an infix reverse-order function application operator:
\texttt{a.b.c} $\equiv$ \texttt{c(b(a))}.

Perhaps arity polymorphism should be renounced in favour of default
arguments?  Note that default arguments need to be respected in either
mode.  \Eg,  \texttt{reverse}/3 could have a default of \texttt{[]} for the third
argument; then \texttt{reverse(AB, BA)} would mean \texttt{reverse(AB, BA, [])},
regardless of which, if either, of the arguments is bound at call
time.  This should be OK.

A motivation for this was to make higher-order code work more
smoothly, but it wouldn't.  A higher order term like \texttt{reverse(AB)} is
still ambiguous, because it's not clear whether this is expecting one
or two more arguments.  Still, a default argument syntax might make
code clearer and more compact when multiple arities are provided just
to handle defaults.

Another approach would be to provide some kind of vararg facility.
This could just be a special syntax for passing some or all the
arguments of a predicate as a list.  Worth the complication?

Perhaps the best way to handle user-defined operators would be to use
a conventional grammar-based parser, but specify the nonterminals of
the grammar, and allow users to add new productions for existing
nonterminals.  The same could be done for the lexer, to allow a
library to define the syntax for strings.  This approach would allow
binary prefix operators, distfix operators, and other grammar
extensions.

Juxtaposition should be function/relation application, like Haskell.  If
applying to a tuple is the same as applying to each argument of the tuple,
then p(x,y) = p(x)(y) = p x (y) = p x y.  Nice and clean.

Should the language syntax be C-like, for the C-ness envy factor?  Or maybe
python- or ruby-like, for the clean, lightweight look.


\section{Compilation}

The language should be designed with optimising compilation in mind.  In
particular, runtime type information should be kept to a minimum.  As much of
the program as possible should be monoporphized during compilation.

Transform nondet and multi predicates into two predicates: one that
initialises a cursor, and one that takes a cursor as input and
extracts the next set of bindings and next cursor.  That is, compile
nondeterminism into coroutining.  This will allow cursors to be used
independently.  One example of this would be to turn a conjunction of
nondeterministic goals into a RDBMS-style join operation when the two
nondeterministic goals could be compiled into code that generates
solutions in order.  In some cases, the code to find the next solution
and next cursor could be quite efficient.  \Eg, the code find the next
solution for between/3 would be quite simple:  just increment the
counter.  Similarly for member/2:  take the tail of the list.

The problem is: how can we handle binding of variables appearing
deeply nested inside other terms?  Basically, terms that would need to
be trailed.  The cursor would need to maintain them somehow.  So it
might always be possible to compile nondeterministic code into
\emph{impure} deterministic, but not to \emph{pure} deterministic
code.


\section{Philosophy}

There is no reason to assume the information available to the
reader of a program is limited to what the writer writes.  Having type
and mode information available no doubt makes a program easier to
understand, but this doesn't mean the programmer must write it.  It
could be produced by listing generation tool, or even by a smart program
editor (which would also give this information to the programmer as
she works).

Separate compilation, defined to mean the ability to explicitly
compile one source file at a time and explicitly link together the
object files, is not necessary.  The real goal is to be able to
quickly begin testing code after a small change.  One would usually
prefer to use a tool like \texttt{make} to rebuild an executable than to
manually invoke the compiler on the changed files anyway.  A
language-aware \texttt{make} program could easily avoid recompiling code that
is unchanged (by saving away the token stream of the previously
compiled version of the file).  It could also keep track of what
object code must be regenerated when a particular definition changes.
Therefore, the ``need'' for separate compilation should not be taken as
an excuse for not applying aggressive intermodule analysis and
optimisation.


\section{Multiparadigm}

One principal (and principle) difference between imperative (and object
oriented) programming and declarative programming is handling of destructive
update:  declarative languages don't allow it, and imperative and object
oriented languages couldn't live without it.  \Frege reconciles these by
having it both ways:  destructive update is not permitted, but a few
syntactic constructs are provided that provide much of the convenience of
destructive update.

Of course, one of the advantages of destructive update is efficiency.  \Frege
regains this efficiency by generating code that actually does use destructive
update.  This is achieved by analysing the code to look for opportunities for
transforming non-destructive code into destructive code.  This is a
multivariant analysis:  some uses of an operation might be destructive, while
others are not.  In this case, two different versions of the operation will
be compiled.

The compiler transforms polymorphic operations into monomorphic ones before
compiling.  This is akin to C++'s handling of templates, which is regarded as
causing a significant code explosion.  \Frege avoids this in two ways.
Firstly, the compiler avoids generating multiple copies of the same code by
hashing the code for each procedure it generates.  Whenever it finds it has
generated the same code twice, it throws one copy away and uses the other in
its stead.

Recursion raises a problem, however.  If code is generated bottom-up, then
each distinct procedure can be given a unique ID as it is generated, and a
hash table can record that a procedure with that hash has been generated, and
what its unique ID is, and a separate table maps the procedure name to its
ID.  When a call to a procedure is generated, its unique ID is used in
computing the hash for the calling procedure.  However, the ID for the
procedure currently being compiled will not have been generated yet, so it
cannot be used when handling a recursive call.

The solution is a two pass algorithm.  Firstly, we compute the hashes for all
of the mutually recursive procedures using a hash of the procedure names, or
some other arbitrary value, for the recursive calls.  Then these procedures
are sorted based on their first pass hash values.  Finally, a second pass is
performed in which the hash for recursive calls is (a hash of) each
procedure's serial number in the ordering derived in the first pass.

\section{Compilation}

Since this is a very high level language, a high level of optimisation will
be necessary for it to be practically useful.  The following optimisations
will be important:

\begin{itemize}
\item Deforestation
\item Structure reuse (compile-time garbage collection)
\end{itemize}


\section{Ideas / Notes}

It should be possible to ignore memory issues, such as structure
sharing (aliasing), but also possible to address it when needed.
Currently in pure languages like Mercury, one must either use ordinary
terms and ignore such issues, or address such issues by using a
store.  But the store forces one to manually dereference pointers --
it's like being back in C.  There should be some compromise, that
allows one to \emph{read} such terms without worrying about
dereferencing, but without losing the pure semantics.

A constructor for a type $t$ is simply a function whose range is $t$.  No
need to distinguish them, except for documentation, and to specify mutual
exclusion and exhaustiveness of constructors.

Support communication between nondeterministic alternatives by employing the
semantics:  the meaning of a relation is a set of tuples.  So using a
relation as a value yields a set, which can be used explicitly.  If this set
is taken as a lazy set, then using its elements can automatically coroutine
with generating them.  Of course, this is exactly what happens with the
normal backtracking approach to nondeterminism in current Logic Programming
languages.

This approach also works the other way:  a set of tuples can be passed around
and called in the program like a predicate.


\section{Problems}

How to reconcile mode polymorphism with subtyping?  \Ie, usually one
wants subtyping for inputs (the caller may pass any supertype of the
declared type, the callee can only use operations of the declared
type) and supertyping for outputs (the callee may pass back any
supertype of the declared type, the caller can only use operations of
the declared type).  How can one handle this when the decision between
input and output is made at compile or even runtime?  The key is the
Liskov substitution principle.

This problem arises because we presume that the code that creates the
term must determine its type, but that is not so: the type of a newly
created term could be decided by the caller.  In fact, each predicate
could decide whether only the caller, or only the callee, or neither,
is permitted to use operations of supertypes of the declared argument
type.  For example, a predicate or function interface might specify
one argument to be a supertype of list of char, meaning an term of any
type which implements all the operations of list of char could be
passed in or out.  Another argument might specify that neither the caller
nor the predicate itself may use supertype operations.  Still another
may permit the predicate to use supertype operations, but forbid the
caller from doing so.

Some predicates have multiple solutions, but some solutions are
preferred to others.  For example, a parser may parse many different inputs to
a single parse tree; run backwards, it will produce many strings from
a single parse tree, but one may be preferred.  One way to handle this
would be to have an ``or else'' operator --- like or, but only
considering the second disjunct after the first one.  In conjunction
with some kind of commit or ``pick the first solution'' operator, this
could handle this without losing purity.  But we still need to be
sure that p(X, Y), Y=... does not behave differently than Y = ...,
p(X, Y), so commitment to an earlier solution of an or else operator
must wait until it is certain that no further constraints on that
solution.

How to handle tuples?  Is a triple just a pair one of whose arguments is
another pair?  Is a 1-tuple the same as a scalar?  Do we need to have the
concept of an empty tuple?  Is \tuple{a,\tuple{b,c}} the same as
\tuple{\tuple{a,b},c}?  In some ways, the most convenient approach would
consider $\tuple{a} = a$, and $\tuple{a,\tuple{b,c}} =
\tuple{\tuple{a,b},c}$.  This means we don't need empty tuples, and don't
need to worry about different arities of tuple construction.  Tuples
essentially become flat, ordered, non-homogeneous collections.  But this
means one cannot put a tuple inside another tuple without it losing its
integrity.  Is that a problem?  Another possibility would be to make tuples
be like LISP-style lists:  with an empty tuple constructor and a pair
constructor, with a convention that right-recursive nested tuples are written
as higher-arity tuples.  These could be still be implemented as a flat
ordered, non-homogeneous collections, much the way C structs are.
Finally, we could have different arities of tuples as separate
non-interchangeable things.

\end{document}

% LocalWords:  vararg predicate's nonfailure DCG monoporphized nondet RDBMS
% LocalWords:  coroutining distfix ness supertyping Liskov backquote
% LocalWords:  restrictable
