%  Emacs    : -*- latex -*-
%  File     : design.tex
%  RCS      : $Id: design.tex,v 1.1 2003/03/28 08:42:40 schachte Exp $
%  Author   : Peter Schachte
%  Origin   : Fri Mar 28 19:36:48 2003
%  Purpose  : Discussion document on design of new language
%  Copyright: © 2003 Peter Schachte.  All rights reserved.
%

\documentclass{article}
\begin{document}
\title{Frege Design Document}
\author{Peter Schachte}

\maketitle

\section{Aims}

The aim of this project is to produce a pure, clean logic/functional
programming language simple and intuitive enough to be suited to
teaching as a first or second language, yet powerful enough for
serious software engineering.

The language should be simple.  There should be very little built in;
most of it should be in libraries.  Ideally, all normal datatypes
should be provided by libraries.  Probably function and predicate
(higher order) types would have to be built in.  This raises
interesting issues for manifest constants.  If integers aren't built
in, how can the compiler handle constants like 42?  You can't define
the integers as an enumeration!  Perhaps 42 and 3.14159 could be
handled by the parser as identifiers?  Strings would seem like an even
bigger problem.


\section{Philosophy}

There is no reason to assume the the information available to the
reader of a program is limited to what the writer writes.  Having type
and mode information available no doubt makes a program easier to
understand, but this doesn't mean the programmer must write it.  This
could be done by listing generation tool, or even by a smart program
editor (which would also give this information to the programmer as
she works).

Separate compilation, defined to mean the ability to explicitly
compile one source file at a time and explicitly link together the
object files, is *not* necessary.  The real goal is to be able to
quickly begin testing code after a small change.  One would usually
prefer to use a tool like 'make' to rebuild an executable than to
manually invoke the compiler on the changed files anyway.  A
language-aware make program could easily avoid recompiling code that
is unchanged (by saving away the token stream of the previously
compiled version of the file).  It could also keep track of what
object code must be regenerated when a particular definition changes.
Therefore, the "need" for separate compilation should not be taken as
an excuse for not applying aggressive intermodule analysis and
optimization.


\section{Types / Classes / Modules}

The language should be strongly typed, with parametric polymorphism
and type inference.

All data types should be abstract.  This would be achieved by treating
constructors (and deconstructors) as ordinary functions.  Thus an
existing type with defined constructors and deconstructors could be
redefined with new functions implementing the previous constructors
and deconstructors.

A type can then be viewed as just the set of functions and predicates
that operate on it.  That is, a type is just a kind of module.  It's
not clear that any other kind of module is needed.  It should also be
possible to define nested modules in a layered kind of way: the
innermost layer would be just the constructor and most primitive
operations.  Layer 1 would be built using only layer 0 facilities.
Layer 2 would use only layer 1, etc.  This would allow maintenance on
one layer at a time, with changes to any layer not affecting the lower
and higher levels as long as the same interfaces can be maintained.

Pushing this idea further, it should then be possible to treat any
type as an interface that any other type could be specified to
implement -- ie, interface inheritance, or type classes.  This should
make it possible to redefine a list as a packed array, and have all
list predicates and functions also work on that type.  It should even
be possible to define input and output streams to be compatible with
lists, so list operations should work for them too.  Read from or
write to a stream by just appending to or from it.  Understanding
determinism is important for this to work, though.

There would have to be a mechanism for declaring mutual exclusion and
exhaustiveness of functions (and predicates).

At the lowest level, it would be necessary to be able to access the
raw data of the type.  Most types would be defined as algebraic types,
but sometimes something more low-level would be best.  Probably the
most flexible primitive building block would be the packed array of
bits.  This would allow a Boolean or character type to be 1 or 8 or 16
bits.  There would also need to be an address type and an addressible
array of bits, which would be aligned as necessary to make it
efficiently addressable.  This should allow lists of characters to be
represented as packed arrays of byes.  Note that this general approach
makes it possible in general to treat foreign data as native types.

Garbage collection would also need to be considered in the design.
The garbage collection interface for the low level facility should be
kept as flexible as possible to allow users to do various clever
things with foreign data, and still allow different implementations to
use different garbage collection strategies.

Each type should be able to specify how instances should be treated
when used in a context where a function or predicate is expected.
This would allow Boolean-valued functions to be used as predicates,
since 'true' used as predicate could be defined to succeed and 'false'
to fail.  It also gives a nice approach to arrays, as an array type
could be defined to apply indexing when used as a function.  That
doesn't handle array element assignment, though.  How can this idea be
generalized?  Perhaps as a coercion system where users can define how
to coerce their type to any other type, including function and
predicate types?  Maybe they want to define how to coerce any other
type to theirs?  But coercion seems directional, which could be a
problem in a logic language.

\section{Modes}

One major principle here is that modes should be simple for users to
understand.  In particular, mode inference should be friendly and
widely used.

A predicate's modes should not be responsible for determining which
modes of a predicate to generate code for.  This should be done by
demand.  So modes are simply a matter of specifying the output
instantiation pattern of arguments given the input pattern.  Eg,
append/3 would specify that on completion, the backbone of the first
argument is ground, and the backbone of the third is if and only if
the backbone of the second is.  Also, the elements of the third list
are all ground iff the elements of the second and third are.

Modes may also be responsible for determining the required initial
instantiation state for a predicate.  For example, the initial
instantiation for the plus/3 predicate (and the +/2 function) might
specify that two of the three arguments must be ground on call.

Modes need to be reflect the structure of types.  Eg, the mode for
append(A,B,C) must say that on success, the spine of A is fully bound,
and the spine of B is iff the spine of C is, and that the elements of
C are fully bound iff those of both A and B are.

It may make sense to follow Mercury in requiring the modes of exported
predicates/functions to be declared.  Others should be inferrable.

It should be possible to write different code for different modes of a
predicate, though it 

\section{Determinism}

Determinism can be at least partially handled by a functional
dependency analysis.  This could be handled by having an instantiation
'definite', meaning ground and lacking any alternatives, which is more
specific than 'ground'.  A predicate's determinism could then be
specified as part of its mode.  The determinism of append(A,B,C), eg,
could be specified as "any one is definite if the other two are."
This, however, doesn't handle possible failure.  For nonfailure, we'd
want to say that append(A,B,C) may fail if both B and C are bound, or
if both A and C are bound.

Note also that this doesn't really tell us everything we want to know
about choicepoints.  For example, this doesn't tell us that
append(A,B,C) will never leave a choicepoint if the spine of A is
fully bound.  Whether B is bound or not is immaterial to choicepoints.

However, it is more powerful than the Mercury system.  It allows us to
consider some arguments of a predicate to be determistic.  For
example, the first argument of

	p(a, b).
	p(a, c).

is definite, while the second is not.  We do not need to store the
first argument in a choicepoint or rebind it when we backtrack into
p/2.  This approach also is not attached to modes; a predicate would
have a single formula describing its determinism.  Furthermore,
definiteness of answers is probably a more natural concept to users
than lack of choicepoints.

\section{Syntax}

There should be an infix reverse order function application operator.
The two most obvious choices are '.' and '|'.  For example, a.b would
mean apply function b to object a, and a.b.c would apply b to a, and
apply c to the value of that.  This works nicely when a is a variable
and b and c are deconstructor functions.  It also behaves much like a
unix pipeline, which is why '|' might be a good choice of operator.
But a|b|c doesn't look that good.

Perhaps arity polymorphism should be renounced in favor of default
arguments?  Note that default arguments need to be respected in either
mode.  Eg,  reverse/3 could have a default of [] for the third
argument; then reverse(AB, BA) would mean reverse(AB, BA, []),
regardless of which, if either, of the arguments is bound at call
time.  This should be OK.

A motivation for this was to make higher-order code work more
smoothly, but it wouldn't.  A higher order term like reverse(AB) is
still ambiguous, because it's not clear whether this is expecting one
or two more arguments.  Still, a default argument syntax might make
code clearer and more compact when multiple arities are provided just
to handle defaults.

Another approach would be to provide some kind of vararg facility.
This could just be a special syntax for passing some or all the
arguments of a predicate as a list.  Worth the complication?

Perhaps the best way to handle user-defined operators would be to use
a conventional grammar-based parser, but specify the nonterminals of
the grammar, and allow users to add new productions for existing
nonterminals.  The same could be done for the lexer, to allow a
library to define the syntax for strings.  This approach would allow
binary prefix operators, distfix operators, and other grammar
extensions.

Should the language syntax be C-like, for the C-ness envy factor?


\section{Compilation}

The language should be designed with optimizing compilation in mind.  In
particular, runtime type information should be kept to a minimum.  As much of
the program as possible should be monoporphized during compilation.


\section{Ideas / Notes}

It should be possible to ignore memory issues, such as structure
sharing (aliasing), but also possible to address it when needed.
Currently in pure languages like Mercury, one must either use ordinary
terms and ignore such issues, or address such issues by using a
store.  But the store forces one to manually dereference pointers --
it's like being back in C.  There should be some compromise, that
allows one to to *read* such terms without worrying about
dereferencing, but without losing the pure semantics.


\section{Problems}

How to reconcile mode polymorphism with subtyping?  Ie, usually one
wants covariance for inputs and contravariance for outputs; how can
one handle this when the decision between input and output is made at
compile or even runtime?  The key is the Liskov substitution principle.
\end{document}
