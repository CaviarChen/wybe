%  Emacs    : -*- latex -*-
%  File     : design.tex
%  RCS      : $Id: design.tex,v 1.6 2007/04/10 14:22:17 schachte Exp $
%  Author   : Peter Schachte
%  Origin   : Fri Mar 28 19:36:48 2003
%  Purpose  : Discussion document on design of new language
%  Copyright: © 2003 Peter Schachte.  All rights reserved.
%

\documentclass{article}
\usepackage{xspace}

\newcommand{\frege}{\textsf{Frege}\xspace}
\newcommand{\Frege}{\textsf{Frege}\xspace}

\newcommand{\ie}{\emph{i.e.}\xspace}
\newcommand{\eg}{\emph{e.g.}\xspace}
\newcommand{\Ie}{\emph{I.e.}\xspace}
\newcommand{\Eg}{\emph{E.g.}\xspace}


\begin{document}
\title{\frege Design Document}
\author{Peter Schachte}

\maketitle

\section{Aims}

The aim of this project is to produce a pure, clean logic/functional
programming language simple and intuitive enough to be suited to
teaching as a first or second language, yet powerful enough for
serious software engineering.
To fit the range of audiences the language is intended for, it must
be possible to use subsets of the language without the interference of other
language features.

From the standpoint of software engineering, the language must support
separation of concerns:  it must be possible to define each aspect of a
system in a separate software component.  Furthermore, each aspect should be
separately maintainable and, where appropriate, reusable.

The current working name for the language is \frege, for
\textbf{F}unctions, \textbf{RE}lations, and pervasive \textbf{GE}nerics.


\section{Types / Classes / Modules}

The language should be strongly typed, with parametric polymorphism
and type inference.

All data types should be abstract.  This would be achieved by treating
constructors (and deconstructors) as ordinary functions.  Thus an
existing type with defined constructors and deconstructors could be
redefined with new functions implementing the previous constructors
and deconstructors.

A type can then be viewed as just the set of functions and predicates
that operate on it.  That is, a type is just a kind of module.  It's
not clear whether any other kind of module is needed.  It should also be
possible to define nested modules in a layered kind of way: the
innermost layer would be just the constructor and most primitive
operations.  Layer 1 would be built using only layer 0 facilities.
only Layer 2 would use layer 1, etc.  This would allow maintenance on
one layer at a time, with changes to any layer not affecting the lower
and higher levels as long as the same interfaces can be maintained.

\Frege supports three approaches to layering:  embedding, extension, and
delegation.  Embedding one module within another hides it from any other
module.  When the interface of an embedded module is changed, only the
embedding module needs to be updated.  The price of this protection is that
no other module may directly use the inner module.

Extending a module defines a new module that provides the full interface of
the extended module, possibly plus some extra features.  This is akin to
inheritance in object oriented languages, and indeed extension allows the
extension module to override features of the extended module by defining its
own implementations.

Delegation permits a module to conveniently specify that features of
related modules should also be considered features of the new module.

Pushing these ideas a little further, it is also possible to treat any
module as an interface that any other module could be specified to
implement --- \ie, interface inheritance, or type classes.  This should
make it possible to define a new list module that implements a list as a
packed array, and have all predicates and functions that operate on lists
also work on that type.  It should even
be possible to define input and output streams to be compatible with
lists, so list operations should work for them too.  Read from or
write to a stream by just appending to or from it.

Thus a module may declare that it \texttt{extends} or \texttt{implements}
another module.  Also, a unary function may declare that it
\texttt{delegates} a number of features.  These features must be members of
the range of that function, and are thereby also made features of the current
module.  Finally, a module may be declared inside another module, in which
case the features of the inner module cannot be used outside of the outer
one.  However, any features defined in the inner module, or defined through
delegation, may be made public by the (outer) module.

Understanding determinism is important for this to work, though.
Therefore, there would have to be a mechanism for declaring mutual
exclusion and exhaustiveness of functions (and predicates).

At the lowest level, it would be necessary to be able to access the
raw data of the type.  Most types would be defined as algebraic types,
but sometimes something more low-level would be best.
In fact, the algebraic type facility should be defined in a
library; users could define their own type facilities.  \emph{E.g},
there could be a C-compatible type system in a library, which would
lay out types compatibly with the way C does, allowing a C type to be
treated as a native \frege type.

Probably the
most flexible primitive building block would be the \texttt{bits}($n$)
type, specifying a packed array of $n$ bits.
This would allow a Boolean or character type to be 1 or 8 or 16
bits.  There would also need to be an \texttt{address} type, at least
large enough to
hold the address of an address, and a addressable
array of bits, which would be aligned as necessary to make it
efficiently addressable.  This should allow lists of characters to be
represented as packed arrays of byes.

There should be a sizeof function returning the size of any type.  In
particular, this can be used to find the size of an address.  
This needs to be
generalised to allow different approaches for addressing characters
and integers/addresses.

Garbage collection would also need to be considered in the design.
The garbage collection interface for the low level facility should be
kept as flexible as possible to allow users to do various clever
things with foreign data, and still allow different implementations to
use different garbage collection strategies.

Each type should be able to specify how instances should be treated
when used in a context where a function or predicate is expected.
This would allow Boolean-valued functions to be used as predicates,
since \texttt{true} used as predicate could be defined to succeed and \texttt{false}
to fail.  It also gives a nice approach to arrays, as an array type
could be defined to apply indexing when used as a function.  That
doesn't handle array element assignment, though.  How can this idea be
generalised?  Perhaps as a coercion system where users can define how
to coerce their type to any other type, including function and
predicate types?  Maybe they want to define how to coerce any other
type to theirs?  But coercion seems directional, which could be a
problem in a logic language.

The language itself should be simple.  There should be very little built in;
most of it should be in libraries.  Ideally, all datatypes that would
normally be considered primitive would be provided by libraries,
which would implement them through the foreign interface.
Probably function and predicate
(higher order) types would have to be built in.  This raises
interesting issues for manifest constants.  If integers aren't built
in, how can the compiler handle constants like 42?  You can't define
the integers as an enumeration!  This could be handled by allowing each type
to define a reader method to convert from a token to a member of the type.

One solution would be to define the language lexically to consist of a
sequence of tokens, where a token is a sequence of characters.  Each token
would be one of the following:
\begin{itemize}
\item one or more alphanumeric characters (including underscore);
\item one open or close parenthesis, bracket, or brace character;
\item a single or double quote or backquote, followed by any number of
  characters up to the matching quote; or
\item a \emph{here} string, described below; or
\item one or more punctuation characters (anything but whitespace not covered
  above).
\end{itemize}
Within a quoted token, a backslash character is treated much as in C:  if the
following character is a letter, it is a character escape; otherwise it is
treated literally; in particular, backslash may be used to escape a quote
character.  Also, a backslash as the last character on a line indicates that
the end-of-line and all following whitespace characters should be ignored.

A \emph{here} string begins with a \emph{here quote} and ends with an
identical here quote, with all intervening text completely uninterpreted.  A
\emph{here quote} is any sequence of non-whitespace characters enclosed
between backslash characters.

This approach would seem to create problems for type inference:  manifest
constants would no longer indicate their types.

\section{Modes}

One major principle here is that modes should be simple for users to
understand.  In particular, mode inference should be friendly and
widely used.

A predicate's modes should not be responsible for determining which
modes of a predicate to generate code for.  This should be done by
demand.  So modes are simply a matter of specifying the output
instantiation pattern of arguments given the input pattern.  \Eg,
\texttt{append}/3 would specify that on completion, the backbone of the first
argument is ground, and the backbone of the third is if and only if
the backbone of the second is.  Also, the elements of the third list
are all ground iff the elements of the first and second are.

Modes may also be responsible for determining the required initial
instantiation state for a predicate.  For example, the initial
instantiation for the \texttt{plus}/3 predicate (and the \texttt{+}/2 function) might
specify that two of the three arguments must be ground on call.

Modes need to be reflect the structure of types.  \Eg, the mode for
\texttt{append(A,B,C)} must say that on success, the spine of \texttt{A} is fully bound,
and the spine of \texttt{B} is iff the spine of \texttt{C} is, and that the elements of
\texttt{C} are fully bound iff those of both \texttt{A} and \texttt{B} are.

It may make sense to follow Mercury in requiring the modes of exported
predicates/functions to be declared.  Others should be inferable.

It should be possible to write different code for different modes of a
predicate.

Perhaps it should be possible to specify what Java would call static
data members for types, \ie, data lexically scoped to the type but
having the lifetime of the program.  For example, to implement a CLP
real variable
type, one needs a constraint store.  To do this properly, this
constraint store needs to be threaded throughout the code.  This leads
to the wrong interface for the primitive operations:  \texttt{+} would
relate three real variables and two constraint stores.  From the
standpoint of modularity, the constraint store should be considered
local to the implementation of the real type.  This gives the
primitive operations the desired interface.  However, we need some
assurances that allowing this will not affect the desired properties
of conjunction; \eg, \texttt{D = A + B, E = D + C} should behave the
same as \texttt{E = D + C, D = A + B}.

An extension of this idea would be to permit conjunction to lose the
desired properties, but then require the use of a special syntax when
using those predicates (perhaps calls to such predicates would have to
be enclosed in braces).  This would give a form of hidden state
threading.  It would be nice to use this for grammars \emph{\`a la}
DCGs, but there would need to be a way to define types that hide state
and make different predicates use different ones of these types.  \Eg,
some clauses would be written as normal 2-extra-argument DCGs, but
others may want to be written as 4-extra-argument DCGs producing an
output text and allowing substitutions.  Perhaps type inheritance
could do this:  define the grammar inside a type that ``extends'' the
DCG type or the substitution grammar type.


\section{Determinism}

Determinism can be at least partially handled by a functional
dependency analysis.  This could be handled by having an instantiation
\texttt{definite}, meaning ground and lacking any alternatives, which is more
specific than \texttt{ground}.  A predicate's determinism could then be
specified as part of its mode.  The determinism of \texttt{append(A,B,C)}, \eg,
could be specified as ``any one is definite if the other two are.''
This, however, doesn't handle possible failure.  For nonfailure, we'd
want to say that \texttt{append(A,B,C)} may fail if both \texttt{B} and \texttt{C} are bound, or
if both \texttt{A} and \texttt{C} are bound.

Note also that this doesn't really tell us everything we want to know
about choicepoints.  For example, this doesn't tell us that
\texttt{append(A,B,C)} will never leave a choicepoint if the spine of \texttt{A} is
fully bound.  Whether \texttt{B} is bound or not is immaterial to choicepoints.

However, it is more powerful than the Mercury system.  It allows us to
consider some arguments of a predicate to be deterministic.  For
example, the first argument of

\begin{verbatim}
	p(a, b).
	p(a, c).
\end{verbatim}

is definite, while the second is not.  We do not need to store the
first argument in a choicepoint or rebind it when we backtrack into
\texttt{p}/2.  This approach also is not attached to modes; a predicate would
have a single formula describing its determinism.  Furthermore,
definiteness of answers is probably a more natural concept to users
than lack of choicepoints.


\section{Syntax}

There should be an infix reverse order function application operator.
The two most obvious choices are \verb@.@ and \verb@|@.  For example, \verb@a.b@ would
mean apply function \verb@b@ to object \verb@a@, and \verb@a.b.c@ would apply \verb@b@ to \verb@a@, and
apply \verb@c@ to the value of that.  This works nicely when a is a variable
and \verb@b@ and \verb@c@ are deconstructor functions.  It also behaves much like a
unix pipeline, which is why \verb@|@ might be a good choice of operator.
But \verb@a|b|c@ doesn't look that good.

Perhaps arity polymorphism should be renounced in favour of default
arguments?  Note that default arguments need to be respected in either
mode.  \Eg,  \texttt{reverse}/3 could have a default of \texttt{[]} for the third
argument; then \texttt{reverse(AB, BA)} would mean \texttt{reverse(AB, BA, [])},
regardless of which, if either, of the arguments is bound at call
time.  This should be OK.

A motivation for this was to make higher-order code work more
smoothly, but it wouldn't.  A higher order term like \texttt{reverse(AB)} is
still ambiguous, because it's not clear whether this is expecting one
or two more arguments.  Still, a default argument syntax might make
code clearer and more compact when multiple arities are provided just
to handle defaults.

Another approach would be to provide some kind of vararg facility.
This could just be a special syntax for passing some or all the
arguments of a predicate as a list.  Worth the complication?

Perhaps the best way to handle user-defined operators would be to use
a conventional grammar-based parser, but specify the nonterminals of
the grammar, and allow users to add new productions for existing
nonterminals.  The same could be done for the lexer, to allow a
library to define the syntax for strings.  This approach would allow
binary prefix operators, distfix operators, and other grammar
extensions.

Should the language syntax be C-like, for the C-ness envy factor?


\section{Compilation}

The language should be designed with optimising compilation in mind.  In
particular, runtime type information should be kept to a minimum.  As much of
the program as possible should be monoporphized during compilation.

Transform nondet and multi predicates into two predicates: one that
initialises a cursor, and one that takes a cursor as input and
extracts the next set of bindings and next cursor.  That is, compile
nondeterminism into coroutining.  This will allow cursors to be used
independently.  One example of this would be to turn a conjunction of
nondeterministic goals into a RDBMS-style join operation when the two
nondeterministic goals could be compiled into code that generates
solutions in order.  In some cases, the code to find the next solution
and next cursor could be quite efficient.  \Eg, the code find the next
solution for between/3 would be quite simple:  just increment the
counter.  Similarly for member/2:  take the tail of the list.

The problem is: how can we handle binding of variables appearing
deeply nested inside other terms?  Basically, terms that would need to
be trailed.  The cursor would need to maintain them somehow.  So it
might always be possible to compile nondeterministic code into
\emph{impure} deterministic, but not to \emph{pure} deterministic
code.


\section{Philosophy}

There is no reason to assume the information available to the
reader of a program is limited to what the writer writes.  Having type
and mode information available no doubt makes a program easier to
understand, but this doesn't mean the programmer must write it.  It
could be produced by listing generation tool, or even by a smart program
editor (which would also give this information to the programmer as
she works).

Separate compilation, defined to mean the ability to explicitly
compile one source file at a time and explicitly link together the
object files, is not necessary.  The real goal is to be able to
quickly begin testing code after a small change.  One would usually
prefer to use a tool like \texttt{make} to rebuild an executable than to
manually invoke the compiler on the changed files anyway.  A
language-aware \texttt{make} program could easily avoid recompiling code that
is unchanged (by saving away the token stream of the previously
compiled version of the file).  It could also keep track of what
object code must be regenerated when a particular definition changes.
Therefore, the ``need'' for separate compilation should not be taken as
an excuse for not applying aggressive intermodule analysis and
optimisation.


\section{Ideas / Notes}

It should be possible to ignore memory issues, such as structure
sharing (aliasing), but also possible to address it when needed.
Currently in pure languages like Mercury, one must either use ordinary
terms and ignore such issues, or address such issues by using a
store.  But the store forces one to manually dereference pointers --
it's like being back in C.  There should be some compromise, that
allows one to \emph{read} such terms without worrying about
dereferencing, but without losing the pure semantics.


\section{Problems}

How to reconcile mode polymorphism with subtyping?  \Ie, usually one
wants subtyping for inputs (the caller may pass any supertype of the
declared type, the callee can only use operations of the declared
type) and supertyping for outputs (the callee may pass back any
supertype of the declared type, the caller can only use operations of
the declared type).  How can one handle this when the decision between
input and output is made at compile or even runtime?  The key is the
Liskov substitution principle.

This problem arises because we presume that the code that creates the
term must determine its type, but that is not so: the type of a newly
created term could be decided by the caller.  In fact, each predicate
could decide whether only the caller, or only the callee, or neither,
is permitted to use operations of supertypes of the declared argument
type.  For example, a predicate or function interface might specify
one argument to be a supertype of list of char, meaning an term of any
type which implements all the operations of list of char could be
passed in or out.  Another argument might specify that neither the caller
nor the predicate itself may use supertype operations.  Still another
may permit the predicate to use supertype operations, but forbid the
caller from doing so.


Some predicates have multiple solutions, but some solutions are
preferred to others.  For example, a parser may parse many different inputs to
a single parse tree; run backwards, it will produce many strings from
a single parse tree, but one may be preferred.  One way to handle this
would be to have an ``or else'' operator --- like or, but only
considering the second disjunct after the first one.  In conjunction
with some kind of commit or ``pick the first solution'' operator, this
could handle this without losing purity.  But we still need to be
sure that p(X, Y), Y=... does not behave differently than Y = ...,
p(X, Y), so commitment to an earlier solution of an or else operator
must wait until it is certain that no further constraints on that
solution.

\end{document}

% LocalWords:  vararg predicate's nonfailure DCG monoporphized nondet RDBMS
% LocalWords:  coroutining distfix ness supertyping Liskov backquote
